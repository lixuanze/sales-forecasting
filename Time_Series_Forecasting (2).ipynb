{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dofC9fblwC4M"
      },
      "source": [
        "# **Section 1: Data Loading**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks7fW86LwGRE"
      },
      "source": [
        "## **1.1: Fidelity's Sales Prices**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlGflxoshJgD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKHtdO2p0ivL"
      },
      "outputs": [],
      "source": [
        "cd /content/drive/MyDrive/Fidelity/Sales Forecasting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "sales_data = pd.read_csv(\"/content/drive/MyDrive/Fidelity/Sales Forecasting/TTM_history_2022Jan.csv\", index_col=False)"
      ],
      "metadata": {
        "id": "qQZq4eHEQWom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_data_gan = sales_data.rename({'TTM Net Sales': 'Close'}, axis = 1)"
      ],
      "metadata": {
        "id": "5zU-w4MPUdX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_data_gan = sales_data_gan.drop([\"Gross Purchase\", \"Gross Redemption\"], axis = 1)"
      ],
      "metadata": {
        "id": "HZqGEp9Mwo0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_data_gan = sales_data_gan.drop([\"Gross Purchase\", \"Gross Redemption\", 'Domestic Balanced 1Y PB', 'Domestic Balanced 3Y PB', 'Domestic Balanced 5Y PB', 'Domestic Equity 1Y PB', 'Domestic Equity 3Y PB', 'Domestic Equity 5Y PB', 'Domestic Fixed Income 1Y PB', 'Domestic Fixed Income 3Y PB', 'Domestic Fixed Income 5Y PB', 'Global & High Yield Fixed Income 1Y PB', 'Global & High Yield Fixed Income 3Y PB', 'Global & High Yield Fixed Income 5Y PB', 'Global & International Equity 1Y PB', 'Global & International Equity 3Y PB', 'Global & International Equity 5Y PB', 'Global Balanced 1Y PB', 'Global Balanced 3Y PB', 'Global Balanced 5Y PB', 'U.S. Equity 1Y PB', 'U.S. Equity 3Y PB', 'U.S. Equity 5Y PB', 'FIC 1Y PB', 'FIC 3Y PB', 'FIC 5Y PB', 'S&P500 1Y', 'S&P500 3Y', 'S&P500 5Y', 'S&PTSX 1Y', 'S&PTSX 3Y', 'S&PTSX 5Y'], axis = 1)"
      ],
      "metadata": {
        "id": "2rob1fMFUpZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_data_gan = sales_data_gan[sales_data_gan['Close'] >= 0]"
      ],
      "metadata": {
        "id": "Cx0Vc0hUmWK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQVuamMtwdgq"
      },
      "source": [
        "# **Section 2: Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOk7Y3-qXqkL"
      },
      "source": [
        "## **2.0: Baseline Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEcLwQd5vD2X"
      },
      "source": [
        "### **2.0.1: LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ELq2r2nvNFY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "\n",
        "from numpy import *\n",
        "from math import sqrt\n",
        "from pandas import *\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.layers import BatchNormalization, Embedding, TimeDistributed, LeakyReLU\n",
        "from tensorflow.keras.layers import LSTM, GRU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from matplotlib import pyplot\n",
        "from pickle import load\n",
        "\n",
        "X_train = np.load(\"X_train.npy\", allow_pickle=True)\n",
        "y_train = np.load(\"y_train.npy\", allow_pickle=True)\n",
        "X_test = np.load(\"X_test.npy\", allow_pickle=True)\n",
        "y_test = np.load(\"y_test.npy\", allow_pickle=True)\n",
        "yc_train = np.load(\"yc_train.npy\", allow_pickle=True)\n",
        "yc_test = np.load(\"yc_test.npy\", allow_pickle=True)\n",
        "\n",
        "#Parameters\n",
        "LR = 0.001\n",
        "BATCH_SIZE = 64\n",
        "N_EPOCH = 50\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "feature_size = X_train.shape[2]\n",
        "output_dim = y_train.shape[1]\n",
        "\n",
        "def basic_lstm(input_dim, feature_size):\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units= 128), input_shape=(input_dim, feature_size)))\n",
        "    model.add(Dense(64))\n",
        "    model.add(Dense(units=output_dim))\n",
        "    model.compile(optimizer=Adam(lr = LR), loss='mse')\n",
        "    history = model.fit(X_train, y_train, epochs=N_EPOCH, batch_size=BATCH_SIZE, validation_data=(X_test, y_test),\n",
        "                        verbose=2, shuffle=False)\n",
        "\n",
        "    pyplot.plot(history.history['loss'], label='train')\n",
        "    pyplot.plot(history.history['val_loss'], label='validation')\n",
        "    pyplot.legend()\n",
        "    pyplot.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "model = basic_lstm(input_dim, feature_size)\n",
        "model.save('LSTM_3to1.h5')\n",
        "print(model.summary())\n",
        "\n",
        "yhat = model.predict(X_test, verbose=0)\n",
        "#print(yhat)\n",
        "\n",
        "rmse = sqrt(mean_squared_error(y_test, yhat))\n",
        "print(rmse)\n",
        "\n",
        "# %% --------------------------------------- Plot the TRAIN result  -----------------------------------------------------------------\n",
        "## TRAIN DATA\n",
        "def plot_traindataset_result(X_train, y_train):\n",
        "\n",
        "\n",
        "    train_yhat = model.predict(X_train, verbose=0)\n",
        "\n",
        "    X_scaler = load(open('X_scaler.pkl', 'rb'))\n",
        "    y_scaler = load(open('y_scaler.pkl', 'rb'))\n",
        "    train_predict_index = np.load(\"train_predict_index.npy\", allow_pickle=True)\n",
        "\n",
        "    rescaled_real_y = y_scaler.inverse_transform(y_train)\n",
        "    rescaled_predicted_y = y_scaler.inverse_transform(train_yhat)\n",
        "\n",
        "    predict_result = pd.DataFrame()\n",
        "    for i in range(rescaled_predicted_y.shape[0]):\n",
        "        y_predict = pd.DataFrame(rescaled_predicted_y[i], columns=[\"predicted_price\"],\n",
        "                                 index=train_predict_index[i:i + output_dim])\n",
        "        predict_result = pd.concat([predict_result, y_predict], axis=1, sort=False)\n",
        "    #\n",
        "    real_price = pd.DataFrame()\n",
        "    for i in range(rescaled_real_y.shape[0]):\n",
        "        y_train = pd.DataFrame(rescaled_real_y[i], columns=[\"real_price\"],\n",
        "                               index=train_predict_index[i:i + output_dim])\n",
        "        real_price = pd.concat([real_price, y_train], axis=1, sort=False)\n",
        "\n",
        "    predict_result['predicted_mean'] = predict_result.mean(axis=1)\n",
        "    real_price['real_mean'] = real_price.mean(axis=1)\n",
        "    #\n",
        "    # Plot the predicted result\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    plt.plot(real_price[\"real_mean\"])\n",
        "    plt.plot(predict_result[\"predicted_mean\"], color='r')\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Stock price\")\n",
        "    plt.legend((\"Real price\", \"Predicted price\"), loc=\"upper left\", fontsize=16)\n",
        "    plt.title(\"The result of Training\", fontsize=20)\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate RMSE\n",
        "    predicted = predict_result[\"predicted_mean\"]\n",
        "    real = real_price[\"real_mean\"]\n",
        "    RMSE = np.sqrt(mean_squared_error(predicted, real))\n",
        "    #print('-- Train RMSE -- ', RMSE)\n",
        "\n",
        "    return RMSE\n",
        "\n",
        "# %% --------------------------------------- Plot the TEST result  -----------------------------------------------------------------\n",
        "def plot_testdataset_result(X_test, y_test):\n",
        "\n",
        "    test_yhat = model.predict(X_test, verbose=0)\n",
        "    y_scaler = load(open('y_scaler.pkl', 'rb'))\n",
        "    test_predict_index = np.load(\"test_predict_index.npy\", allow_pickle=True)\n",
        "\n",
        "    rescaled_real_y = y_scaler.inverse_transform(y_test)\n",
        "    rescaled_predicted_y = y_scaler.inverse_transform(test_yhat)\n",
        "\n",
        "    predict_result = pd.DataFrame()\n",
        "    for i in range(rescaled_predicted_y.shape[0]):\n",
        "        y_predict = pd.DataFrame(rescaled_predicted_y[i], columns=[\"predicted_price\"],\n",
        "                                 index=test_predict_index[i:i + output_dim])\n",
        "        predict_result = pd.concat([predict_result, y_predict], axis=1, sort=False)\n",
        "\n",
        "    real_price = pd.DataFrame()\n",
        "    for i in range(rescaled_real_y.shape[0]):\n",
        "        y_train = pd.DataFrame(rescaled_real_y[i], columns=[\"real_price\"],\n",
        "                               index=test_predict_index[i:i + output_dim])\n",
        "        real_price = pd.concat([real_price, y_train], axis=1, sort=False)\n",
        "\n",
        "    predict_result['predicted_mean'] = predict_result.mean(axis=1)\n",
        "    real_price['real_mean'] = real_price.mean(axis=1)\n",
        "\n",
        "    Input_Before = '2020-01-01'\n",
        "    predict_result = predict_result.loc[predict_result.index < Input_Before]\n",
        "    real_price = real_price.loc[real_price.index < Input_Before]\n",
        "\n",
        "    print(predict_result.tail(10))\n",
        "\n",
        "    # Plot the predicted result\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    plt.plot(real_price[\"real_mean\"])\n",
        "    plt.plot(predict_result[\"predicted_mean\"], color='r')\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Stock price\")\n",
        "    plt.legend((\"Real price\", \"Predicted price\"), loc=\"upper left\", fontsize=16)\n",
        "    plt.title(\"The result of Testing\", fontsize=20)\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate RMSE\n",
        "    predicted = predict_result[\"predicted_mean\"]\n",
        "    real = real_price[\"real_mean\"]\n",
        "    RMSE = np.sqrt(mean_squared_error(predicted, real))\n",
        "    return RMSE\n",
        "\n",
        "def plot_testdataset_with2020_result(X_test, y_test):\n",
        "    test_yhat = model.predict(X_test, 1, verbose=0)\n",
        "\n",
        "    y_scaler = load(open('y_scaler.pkl', 'rb'))\n",
        "    test_predict_index = np.load(\"test_predict_index.npy\", allow_pickle=True)\n",
        "    rescaled_real_y = y_scaler.inverse_transform(y_test)\n",
        "    rescaled_predicted_y = y_scaler.inverse_transform(test_yhat)\n",
        "\n",
        "    predict_result = pd.DataFrame()\n",
        "    for i in range(rescaled_predicted_y.shape[0]):\n",
        "        y_predict = pd.DataFrame(rescaled_predicted_y[i], columns=[\"predicted_price\"],\n",
        "                                 index=test_predict_index[i:i + output_dim])\n",
        "        predict_result = pd.concat([predict_result, y_predict], axis=1, sort=False)\n",
        "\n",
        "    real_price = pd.DataFrame()\n",
        "    for i in range(rescaled_real_y.shape[0]):\n",
        "        y_train = pd.DataFrame(rescaled_real_y[i], columns=[\"real_price\"],\n",
        "                               index=test_predict_index[i:i + output_dim])\n",
        "        real_price = pd.concat([real_price, y_train], axis=1, sort=False)\n",
        "\n",
        "    predict_result['predicted_mean'] = predict_result.mean(axis=1)\n",
        "    real_price['real_mean'] = real_price.mean(axis=1)\n",
        "\n",
        "    # Plot the predicted result\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    plt.plot(real_price[\"real_mean\"])\n",
        "    plt.plot(predict_result[\"predicted_mean\"], color='r')\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Stock price\")\n",
        "    plt.legend((\"Real price\", \"Predicted price\"), loc=\"upper left\", fontsize=16)\n",
        "    plt.title(\"The result of Testing with 2022\", fontsize=20)\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate RMSE\n",
        "    predicted = predict_result[\"predicted_mean\"]\n",
        "    real = real_price[\"real_mean\"]\n",
        "    RMSE = np.sqrt(mean_squared_error(predicted, real))\n",
        "    #print('-- Test RMSE with 2022 -- ', RMSE)\n",
        "    return RMSE\n",
        "\n",
        "train_RMSE = plot_traindataset_result(X_train, y_train)\n",
        "print(\"----- Train_RMSE_LSTM -----\", train_RMSE)\n",
        "\n",
        "test_RMSE = plot_testdataset_result(X_test, y_test)\n",
        "print(\"----- Test_RMSE_LSTM -----\", test_RMSE)\n",
        "\n",
        "test_with2020_RMSE = plot_testdataset_with2020_result(X_test, y_test)\n",
        "print(\"----- Test_RMSE_LSTM_with2020 -----\", test_with2020_RMSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG4QppG8vV3_"
      },
      "source": [
        "### **2.0.2: GAN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GykblZivjpu"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "#from main.feature import get_all_features\n",
        "from tensorflow.keras.layers import GRU, LSTM, Bidirectional, Dense, Flatten, Conv1D, BatchNormalization, LeakyReLU, Dropout\n",
        "from tensorflow.keras import Sequential\n",
        "from pickle import load\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X_train = np.load(\"X_train.npy\", allow_pickle=True)\n",
        "y_train = np.load(\"y_train.npy\", allow_pickle=True)\n",
        "X_test = np.load(\"X_test.npy\", allow_pickle=True)\n",
        "y_test = np.load(\"y_test.npy\", allow_pickle=True)\n",
        "yc_train = np.load(\"yc_train.npy\", allow_pickle=True)\n",
        "yc_test = np.load(\"yc_test.npy\", allow_pickle=True)\n",
        "\n",
        "\n",
        "def make_generator_model(input_dim, output_dim, feature_size) -> tf.keras.models.Model:\n",
        "    model = Sequential()\n",
        "    model.add(GRU(units=1024, return_sequences = True, input_shape=(input_dim, feature_size),\n",
        "                  recurrent_dropout=0.2))\n",
        "    model.add(GRU(units=512, return_sequences = True, recurrent_dropout=0.2)) # 256, return_sequences = True\n",
        "    model.add(GRU(units=256, recurrent_dropout=0.2)) #, recurrent_dropout=0.1\n",
        "    model.add(Dense(128))\n",
        "    model.add(Dense(64))\n",
        "    model.add(Dense(units=output_dim))\n",
        "    return model\n",
        "\n",
        "def make_discriminator_model():\n",
        "    cnn_net = tf.keras.Sequential()\n",
        "    cnn_net.add(Conv1D(32, input_shape=(4, 1), kernel_size=3, strides=2, padding='same', activation=LeakyReLU(alpha=0.01)))\n",
        "    cnn_net.add(Conv1D(64, kernel_size=5, strides=2, padding='same', activation=LeakyReLU(alpha=0.01)))\n",
        "    cnn_net.add(Conv1D(128, kernel_size=5, strides=2, padding='same', activation=LeakyReLU(alpha=0.01)))\n",
        "    cnn_net.add(Flatten())\n",
        "    cnn_net.add(Dense(220, use_bias=False))\n",
        "    cnn_net.add(LeakyReLU())\n",
        "    cnn_net.add(Dense(220, use_bias=False, activation='relu'))\n",
        "    cnn_net.add(Dense(1, activation='sigmoid'))\n",
        "    return cnn_net\n",
        "\n",
        "\n",
        "model = make_discriminator_model()\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "class GAN:\n",
        "    def __init__(self, generator, discriminator, opt):\n",
        "        self.opt = opt\n",
        "        self.lr = opt[\"lr\"]\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "        self.generator_optimizer = tf.keras.optimizers.Adam(lr=self.lr)\n",
        "        self.discriminator_optimizer = tf.keras.optimizers.Adam(lr=self.lr)\n",
        "        self.batch_size = self.opt['bs']\n",
        "        self.checkpoint_dir = '../training_checkpoints'\n",
        "        self.checkpoint_prefix = os.path.join(self.checkpoint_dir, \"ckpt\")\n",
        "        self.checkpoint = tf.train.Checkpoint(generator_optimizer=self.generator_optimizer,\n",
        "                                              discriminator_optimizer=self.discriminator_optimizer,\n",
        "                                              generator=self.generator,\n",
        "                                              discriminator=self.discriminator)\n",
        "\n",
        "    def discriminator_loss(self, real_output, fake_output):\n",
        "        real_loss = self.cross_entropy(tf.ones_like(real_output), real_output)\n",
        "        fake_loss = self.cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "        total_loss = real_loss + fake_loss\n",
        "        return total_loss\n",
        "\n",
        "    def generator_loss(self, fake_output):\n",
        "        return self.cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, real_x, real_y, yc):\n",
        "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "            generated_data = self.generator(real_x, training=True)\n",
        "            generated_data_reshape = tf.reshape(generated_data, [generated_data.shape[0], generated_data.shape[1], 1])\n",
        "            d_fake_input = tf.concat([tf.cast(generated_data_reshape, tf.float64), yc], axis=1)\n",
        "            real_y_reshape = tf.reshape(real_y, [real_y.shape[0], real_y.shape[1], 1])\n",
        "            d_real_input = tf.concat([real_y_reshape, yc], axis=1)\n",
        "\n",
        "            real_output = self.discriminator(d_real_input, training=True)\n",
        "            fake_output = self.discriminator(d_fake_input, training=True)\n",
        "\n",
        "            gen_loss = self.generator_loss(fake_output)\n",
        "            disc_loss = self.discriminator_loss(real_output, fake_output)\n",
        "\n",
        "        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
        "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
        "\n",
        "        self.generator_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
        "        self.discriminator_optimizer.apply_gradients(\n",
        "            zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
        "        return real_y, generated_data, {'d_loss': disc_loss, 'g_loss': gen_loss}\n",
        "\n",
        "    def train(self, real_x, real_y, yc, opt):\n",
        "        train_hist = {}\n",
        "        train_hist['D_losses'] = []\n",
        "        train_hist['G_losses'] = []\n",
        "        train_hist['per_epoch_times'] = []\n",
        "        train_hist['total_ptime'] = []\n",
        "\n",
        "        epochs = opt[\"epoch\"]\n",
        "        for epoch in range(epochs):\n",
        "            start = time.time()\n",
        "            real_price, fake_price, loss = self.train_step(real_x, real_y, yc)\n",
        "\n",
        "            G_losses = []\n",
        "            D_losses = []\n",
        "\n",
        "            Real_price = []\n",
        "            Predicted_price = []\n",
        "\n",
        "            D_losses.append(loss['d_loss'].numpy())\n",
        "            G_losses.append(loss['g_loss'].numpy())\n",
        "\n",
        "            Predicted_price.append(fake_price.numpy())\n",
        "            Real_price.append(real_price.numpy())\n",
        "\n",
        "            # Save the model every 15 epochs\n",
        "            if (epoch + 1) % 15 == 0:\n",
        "                tf.keras.models.save_model(generator, 'gen_model_3_1_%d.h5' % epoch)\n",
        "                self.checkpoint.save(file_prefix=self.checkpoint_prefix + f'-{epoch}')\n",
        "                print('epoch', epoch + 1, 'd_loss', loss['d_loss'].numpy(), 'g_loss', loss['g_loss'].numpy())\n",
        "            # print('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))\n",
        "            # For printing loss\n",
        "            epoch_end_time = time.time()\n",
        "            per_epoch_ptime = epoch_end_time - start\n",
        "            train_hist['D_losses'].append(D_losses)\n",
        "            train_hist['G_losses'].append(G_losses)\n",
        "            train_hist['per_epoch_times'].append(per_epoch_ptime)\n",
        "\n",
        "        # Reshape the predicted result & real\n",
        "        Predicted_price = np.array(Predicted_price)\n",
        "        Predicted_price = Predicted_price.reshape(Predicted_price.shape[1], Predicted_price.shape[2])\n",
        "        Real_price = np.array(Real_price)\n",
        "        Real_price = Real_price.reshape(Real_price.shape[1], Real_price.shape[2])\n",
        "\n",
        "        plt.plot(train_hist['D_losses'], label='D_loss')\n",
        "        plt.plot(train_hist['G_losses'], label='G_loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        return Predicted_price, Real_price, np.sqrt(mean_squared_error(Real_price, Predicted_price)) / np.mean(\n",
        "            Real_price)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    input_dim = X_train.shape[1]\n",
        "    feature_size = X_train.shape[2]\n",
        "    output_dim = y_train.shape[1]\n",
        "    ## For Bayesian\n",
        "    opt = {\"lr\": 0.00016, \"epoch\": 165, 'bs': 128}\n",
        "\n",
        "    generator = make_generator_model(X_train.shape[1], output_dim, X_train.shape[2])\n",
        "    discriminator = make_discriminator_model()\n",
        "    gan = GAN(generator, discriminator, opt)\n",
        "    Predicted_price, Real_price, RMSPE = gan.train(X_train, y_train, yc_train, opt)\n",
        "\n",
        "# %% --------------------------------------- Plot the result  -----------------------------------------------------------------\n",
        "\n",
        "# Rescale back the real dataset\n",
        "X_scaler = load(open('X_scaler.pkl', 'rb'))\n",
        "y_scaler = load(open('y_scaler.pkl', 'rb'))\n",
        "train_predict_index = np.load(\"train_predict_index.npy\", allow_pickle=True)\n",
        "test_predict_index = np.load(\"test_predict_index.npy\", allow_pickle=True)\n",
        "#dataset_train = pd.read_csv('dataset_train.csv', index_col=0)\n",
        "\n",
        "\n",
        "print(\"----- predicted price -----\", Predicted_price)\n",
        "\n",
        "rescaled_Real_price = y_scaler.inverse_transform(Real_price)\n",
        "rescaled_Predicted_price = y_scaler.inverse_transform(Predicted_price)\n",
        "\n",
        "print(\"----- rescaled predicted price -----\", rescaled_Predicted_price)\n",
        "print(\"----- SHAPE rescaled predicted price -----\", rescaled_Predicted_price.shape)\n",
        "\n",
        "predict_result = pd.DataFrame()\n",
        "for i in range(rescaled_Predicted_price.shape[0]):\n",
        "    y_predict = pd.DataFrame(rescaled_Predicted_price[i], columns=[\"predicted_price\"], index=train_predict_index[i:i+output_dim])\n",
        "    predict_result = pd.concat([predict_result, y_predict], axis=1, sort=False)\n",
        "#\n",
        "real_price = pd.DataFrame()\n",
        "for i in range(rescaled_Real_price.shape[0]):\n",
        "    y_train = pd.DataFrame(rescaled_Real_price[i], columns=[\"real_price\"], index=train_predict_index[i:i+output_dim])\n",
        "    real_price = pd.concat([real_price, y_train], axis=1, sort=False)\n",
        "\n",
        "predict_result['predicted_mean'] = predict_result.mean(axis=1)\n",
        "real_price['real_mean'] = real_price.mean(axis=1)\n",
        "\n",
        "# Plot the predicted result\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.plot(real_price[\"real_mean\"])\n",
        "plt.plot(predict_result[\"predicted_mean\"], color = 'r')\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Stock price\")\n",
        "plt.legend((\"Real price\", \"Predicted price\"), loc=\"upper left\", fontsize=16)\n",
        "plt.title(\"The result of Training\", fontsize=20)\n",
        "plt.show()\n",
        "\n",
        "# Calculate RMSE\n",
        "predicted = predict_result[\"predicted_mean\"]\n",
        "real = real_price[\"real_mean\"]\n",
        "For_MSE = pd.concat([predicted, real], axis = 1)\n",
        "RMSE = np.sqrt(mean_squared_error(predicted, real))\n",
        "print('-- Train RMSE -- ', RMSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmyyUm5rNbVs"
      },
      "source": [
        "### **2.0.3: DDPG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdz1bfPlNd98"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This part of code is to load and preprocess time series data.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "def build_s_a(sequence,n,m):\n",
        "    '''\n",
        "    Args:\n",
        "        sequence: Time series data\n",
        "        n: The number of historical data denoting the current state\n",
        "        m: The number of prediction steps in advance\n",
        "    Return:\n",
        "        state_mat: A matrix contains all states at each time step\n",
        "        best_action: The optimal action based on each state\n",
        "    '''\n",
        "    n_rows = len(sequence)-n-m+1\n",
        "    state_mat = np.zeros((n_rows,n))\n",
        "    best_action = np.zeros(n_rows)\n",
        "    for i in range(n_rows):\n",
        "        state_mat[i] = sequence[i:(i+n)]\n",
        "        best_action[i] = sequence[i+n+m-1]\n",
        "    return state_mat,best_action\n",
        "\n",
        "def normalization(traindata,testdata):\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(traindata)\n",
        "    traindata_scaled = scaler.transform(traindata)\n",
        "    testdata_scaled = scaler.transform(testdata)\n",
        "    return traindata_scaled,testdata_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G28rMdRk0UDg"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The agent of RL algorithm Deep Detrministic Policy Gradient.\n",
        "Both the Actor and Critic neuron networks  adopt three-layer Fully-Connected NN.\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "class DDPG():\n",
        "    def __init__(self,\n",
        "                 n_features,\n",
        "                 a_low,\n",
        "                 a_high,\n",
        "                 learning_rate_actor,\n",
        "                 learning_rate_critic,\n",
        "                 n_actor_hidden,\n",
        "                 n_critic_hidden,\n",
        "                 gamma = 0.9,\n",
        "                 noise_varience = 3,\n",
        "                 soft_replace = 0.1,\n",
        "                 memory_size = 1000,\n",
        "                 batch_size = 128):\n",
        "        self.n_features = n_features             #dimension of states    \n",
        "        self.a_low = a_low                       #The low bound of action sapce\n",
        "        self.a_high = a_high                     #The high bound of action space\n",
        "        self.lr_a = learning_rate_actor          #Learning rate of Actor NN\n",
        "        self.lr_c = learning_rate_critic         #Learning rate of Critic NN\n",
        "        self.n_actor_hidden = n_actor_hidden     #Number of hidden layer neurons in Actor\n",
        "        self.n_critic_hidden = n_critic_hidden   #Number of hidden layer neurons in Critic\n",
        "        self.gamma = gamma                       #Reward discount rate\n",
        "        self.noise_var = noise_varience          #Variance of output action distribution\n",
        "        self.soft_replace = soft_replace         #Update speed of target networks\n",
        "        self.memory_size = memory_size           #Size of experience replay buffer\n",
        "        self.memory = deque(maxlen = self.memory_size)   #Experience replay buffer\n",
        "        self.batch_size = batch_size                     \n",
        "        \n",
        "        self.s = tf.placeholder(dtype = tf.float32,shape = [None,self.n_features])\n",
        "        self.s_ = tf.placeholder(dtype = tf.float32,shape = [None,self.n_features])\n",
        "        self.r = tf.placeholder(dtype = tf.float32,shape = [None,])\n",
        "        self.done = tf.placeholder(dtype = tf.float32,shape = [None,]) # 0 if s_ == terminal else 1\n",
        "        \n",
        "        self.a = self.build_Actor1()\n",
        "        self.a_ = self.build_Actor2()\n",
        "        self.q_sa = self.build_Critic1()      #shape:[None,] \n",
        "        self.q_s_a_ = self.build_Critic2()    #shape:[None,]\n",
        "        \n",
        "        self.curr_a_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                                            scope = 'Actor/Current')\n",
        "        self.targ_a_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                                            scope = 'Actor/Target')\n",
        "        self.curr_c_params= tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                                            scope = 'Critic/Current')\n",
        "        self.targ_c_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                                            scope = 'Critic/Target')\n",
        "        \n",
        "        # Soft replace of Targets NN parameters\n",
        "        self.replace_a_params = [tf.assign(t,(1-self.soft_replace)*t + self.soft_replace*e) \\\n",
        "                                 for (t,e) in zip(self.targ_a_params,self.curr_a_params)]\n",
        "        self.replace_c_params = [tf.assign(t,(1-self.soft_replace)*t + self.soft_replace*e) \\\n",
        "                                 for (t,e) in zip(self.targ_c_params,self.curr_c_params)]\n",
        "        \n",
        "        self.td_error = self.r + self.gamma*self.q_s_a_ - self.q_sa\n",
        "        self.critic_loss = tf.reduce_mean(tf.square(self.td_error))\n",
        "        self.actor_loss = -tf.reduce_mean(self.q_sa)\n",
        "        \n",
        "        self.actor_train_op = tf.train.AdamOptimizer(self.lr_a).minimize(self.actor_loss,\n",
        "                                                    var_list = self.curr_a_params)\n",
        "        self.critic_train_op = tf.train.AdamOptimizer(self.lr_c).minimize(self.critic_loss,\n",
        "                                                     var_list = self.curr_c_params)\n",
        "        \n",
        "        self.learn_step_counter = 0\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "        \n",
        "    \n",
        "    def build_Actor1(self):\n",
        "        '''\n",
        "        Building Current Actor network.\n",
        "        '''\n",
        "        with tf.variable_scope('Actor/Current'):\n",
        "            w_init = tf.random_normal_initializer(0,0.1)\n",
        "            b_init = tf.constant_initializer(0.1)\n",
        "            w1 = tf.get_variable(name = 'w1',shape = [self.n_features,self.n_actor_hidden],\n",
        "                                 dtype = tf.float32,initializer = w_init,\n",
        "                                 trainable = True)\n",
        "            b1 = tf.get_variable('b1',shape = [self.n_actor_hidden,],\n",
        "                                 dtype = tf.float32,initializer = b_init,\n",
        "                                 trainable = True)\n",
        "            w2 = tf.get_variable('w2',shape = [self.n_actor_hidden,1],\n",
        "                                 dtype = tf.float32,initializer = w_init,\n",
        "                                 trainable = True)\n",
        "            b2 = tf.get_variable('b2',shape = [1,],\n",
        "                                 dtype = tf.float32,initializer = b_init,\n",
        "                                 trainable = True)\n",
        "            hidden = tf.nn.relu(tf.matmul(self.s,w1) + b1)\n",
        "            a = tf.matmul(hidden,w2) + b2\n",
        "        return a[:,0]\n",
        "    \n",
        "    def build_Actor2(self):\n",
        "        '''\n",
        "        Building Target Actor network.\n",
        "        '''\n",
        "        with tf.variable_scope('Actor/Target'):\n",
        "            w_init = tf.random_normal_initializer(0,0.1)\n",
        "            b_init = tf.constant_initializer(0.1)\n",
        "            w1 = tf.get_variable('w1',shape = [self.n_features,self.n_actor_hidden],\n",
        "                                 dtype = tf.float32,initializer = w_init,\n",
        "                                 trainable = False)\n",
        "            b1 = tf.get_variable('b1',shape = [self.n_actor_hidden,],\n",
        "                                 dtype = tf.float32,initializer = b_init,\n",
        "                                 trainable = False)\n",
        "            w2 = tf.get_variable('w2',shape = [self.n_actor_hidden,1],\n",
        "                                 dtype = tf.float32,initializer = w_init,\n",
        "                                 trainable = False)\n",
        "            b2 = tf.get_variable('b2',shape = [1,],\n",
        "                                 dtype = tf.float32,initializer = b_init,\n",
        "                                 trainable = False)\n",
        "            hidden = tf.nn.relu(tf.matmul(self.s_,w1) + b1)\n",
        "            a_ = tf.matmul(hidden,w2) + b2\n",
        "        return a_[:,0]\n",
        "    \n",
        "    def build_Critic1(self):\n",
        "        '''\n",
        "        Building Current Critic network.\n",
        "        '''\n",
        "        with tf.variable_scope('Critic/Current'):\n",
        "            w_init = tf.random_normal_initializer(0,0.1)\n",
        "            b_init = tf.constant_initializer(0.1)\n",
        "            w1_s = tf.get_variable('w1_s',shape = [self.n_features,self.n_critic_hidden],\n",
        "                                 dtype = tf.float32,initializer = w_init,\n",
        "                                 trainable = True)\n",
        "            w1_a = tf.get_variable('w1_a',shape = [1,self.n_critic_hidden],\n",
        "                                 dtype = tf.float32,initializer = w_init,\n",
        "                                 trainable = True)\n",
        "            b1 = tf.get_variable('b1',shape = [self.n_critic_hidden,],\n",
        "                                 dtype = tf.float32,initializer = b_init,\n",
        "                                 trainable = True)\n",
        "            w2 = tf.get_variable('w2',shape = [self.n_critic_hidden,1],\n",
        "                                 dtype = tf.float32,initializer = w_init,\n",
        "                                 trainable = True)\n",
        "            b2 = tf.get_variable('b2',shape = [1,],dtype = tf.float32,\n",
        "                                 initializer = b_init,trainable = True)\n",
        "            hidden = tf.nn.relu(tf.matmul(self.s,w1_s) + tf.matmul(self.a[:,np.newaxis],w1_a) + b1)\n",
        "            q_sa = tf.matmul(hidden,w2) + b2\n",
        "        return q_sa[:,0]\n",
        "    \n",
        "    def build_Critic2(self):\n",
        "        '''\n",
        "        Building Target Critic network.\n",
        "        '''\n",
        "        with tf.variable_scope('Critic/Target'):\n",
        "            w_init = tf.random_normal_initializer(0,0.1)\n",
        "            b_init = tf.constant_initializer(0.1)\n",
        "            w1_s = tf.get_variable('w1_s',shape = [self.n_features,self.n_critic_hidden],\n",
        "                                 dtype = tf.float32,initializer = w_init,\n",
        "                                 trainable = False)\n",
        "            w1_a = tf.get_variable('w1_a',shape = [1,self.n_critic_hidden],\n",
        "                                 dtype = tf.float32,initializer = w_init,\n",
        "                                 trainable = False)\n",
        "            b1 = tf.get_variable('b1',shape = [self.n_critic_hidden,],\n",
        "                                 dtype = tf.float32,initializer = b_init,\n",
        "                                 trainable = False)\n",
        "            w2 = tf.get_variable('w2',shape = [self.n_critic_hidden,1],\n",
        "                                 dtype = tf.float32,initializer = w_init,\n",
        "                                 trainable = False)\n",
        "            b2 = tf.get_variable('b2',shape = [1,],dtype = tf.float32,\n",
        "                                 initializer = b_init,trainable = True)\n",
        "            hidden = tf.nn.relu(tf.matmul(self.s_,w1_s) + tf.matmul(self.a_[:,np.newaxis],w1_a) + b1)\n",
        "            q_s_a_ = tf.matmul(hidden,w2) + b2\n",
        "        return q_s_a_[:,0]            \n",
        "    \n",
        "    def choose_action(self,state):\n",
        "        state = np.reshape(state,[-1,self.n_features])\n",
        "        action = self.sess.run(self.a,feed_dict = {self.s:state})\n",
        "        return action\n",
        "    \n",
        "    def store_transition(self,state,action,reward,next_state):\n",
        "        state,next_state = state[np.newaxis,:],next_state[np.newaxis,:]\n",
        "        action,reward = np.array(action),np.array(reward)\n",
        "        action = np.reshape(action,[1,-1])\n",
        "        reward = np.reshape(reward,[1,-1])\n",
        "        \n",
        "        transition = np.concatenate((state,action,reward,next_state),axis = 1)\n",
        "        self.memory.append(transition[0,:])\n",
        "    \n",
        "    def learn(self):\n",
        "        if len(self.memory) == self.memory_size:\n",
        "            if self.learn_step_counter % 200 == 0:\n",
        "                self.sess.run((self.replace_a_params,self.replace_c_params))\n",
        "            self.noise_var *= 0.999\n",
        "            batch = np.array(random.sample(self.memory,self.batch_size))\n",
        "            batch_s = batch[:,:self.n_features]\n",
        "            batch_a = batch[:,self.n_features:(self.n_features + 1)][:,0]\n",
        "            batch_r = batch[:,(self.n_features + 1):(self.n_features + 2)][:,0]\n",
        "            batch_s_ = batch[:,(self.n_features + 2):(self.n_features*2 + 2)]\n",
        "            \n",
        "            self.sess.run(self.actor_train_op,feed_dict = {self.s:batch_s})\n",
        "            self.sess.run(self.critic_train_op,feed_dict = {self.s:batch_s,\n",
        "                                                            self.a:batch_a,\n",
        "                                                            self.s_:batch_s_,\n",
        "                                                            self.r:batch_r})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnU8mDxN0Up2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "#####################  hyper parameters  ####################\n",
        "N_FEATURES = 6\n",
        "A_LOW = 0\n",
        "A_HIGH = 1\n",
        "LR_A = 0.001\n",
        "LR_C = 0.003\n",
        "N_ACTOR_HIDDEN = 30\n",
        "N_CRITIC_HIDDEN = 30\n",
        "MAX_EPISODES = 300\n",
        "MAX_STEPS = 1000\n",
        "\n",
        "GAMMA = 0.9                # 折扣因子\n",
        "TAU = 0.1                 # 软更新因子\n",
        "MEMORY_CAPACITY = 100000    #记忆库大小\n",
        "BATCH_SIZE = 128            #批梯度下降的m\n",
        "#############################################################\n",
        "\n",
        "#Load data \n",
        "data = sales_data['TTM Net Sales']\n",
        "\n",
        "#Build state matrix and best action\n",
        "state,action = build_s_a(data,N_FEATURES, 15)\n",
        "\n",
        "#Data split\n",
        "SPLIT_RATE = 0.94\n",
        "split_index = round(len(state)*SPLIT_RATE)\n",
        "train_s,train_a = state[:split_index],action[:split_index]\n",
        "test_s,test_a = state[split_index:],action[split_index:]\n",
        "\n",
        "#Normalization\n",
        "train_s_scaled,test_s_scaled = normalization(train_s,test_s)\n",
        "A,B = train_a.max(),train_a.min()\n",
        "train_a_scaled,test_a_scaled = (train_a-B)/(A-B),(test_a-B)/(A-B)\n",
        "\n",
        "# Training\n",
        "ddpg = DDPG(N_FEATURES, A_LOW,A_HIGH,LR_A,LR_C,N_ACTOR_HIDDEN,N_CRITIC_HIDDEN)\n",
        "for episode  in range(MAX_EPISODES):\n",
        "    index = np.random.choice(range(len(train_s_scaled) - 1))\n",
        "    s = train_s_scaled[index]\n",
        "    ep_reward = 0\n",
        "    \n",
        "    for step in range(MAX_STEPS):\n",
        "        a = ddpg.choose_action(s)\n",
        "        r = -abs(a-train_a_scaled[index])\n",
        "        ep_reward += r\n",
        "        index += 1\n",
        "        s_ = train_s_scaled[index]\n",
        "        \n",
        "        ddpg.store_transition(s,a,r,s_)\n",
        "        ddpg.learn()\n",
        "        \n",
        "        if (index == len(train_s_scaled)-1) or (step == MAX_STEPS-1):\n",
        "            print('Episode %d : %.2f'%(episode,ep_reward))\n",
        "            break\n",
        "        s = s_\n",
        "\n",
        "# Testing\n",
        "pred = []\n",
        "for i in range(len(test_s_scaled)):\n",
        "    state = test_s_scaled[i]\n",
        "    action = ddpg.choose_action(state)\n",
        "    pred.append(action)\n",
        "\n",
        "pred = [pred[i][0] for i in range(len(test_s_scaled))]\n",
        "pred = pd.Series(pred)\n",
        "pred = pred*(A-B)+B\n",
        "actual = pd.Series(test_a)\n",
        "\n",
        "plt.scatter(pred,test_a,marker = '.')\n",
        "plt.xlabel('Predicted Value')\n",
        "plt.ylabel('Actual value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haALPb0PR3-h"
      },
      "source": [
        "## **2.1: WGAN + Gradient Panelty (Good model)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ad8kRLiyxx7"
      },
      "source": [
        "### **2.1.1: Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-BflVEYR8-n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.dates import DateFormatter\n",
        "import math\n",
        "\n",
        "## import data\n",
        "df = sales_data_gan\n",
        "# sales_data_gan.to_csv(\"Finaldata_with_Fourier.csv\", index=False)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.shape)\n",
        "print(df.columns)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,3))\n",
        "ax.plot(df['Date'], df['Close'], label='Fidelity Sales')\n",
        "ax.set(xlabel=\"Date\",\n",
        "       ylabel=\"USD\",\n",
        "       title=\"Sales\")\n",
        "date_form = DateFormatter(\"%Y\")\n",
        "ax.xaxis.set_major_formatter(date_form)\n",
        "plt.show()\n",
        "\n",
        "# Calculate technical indicators\n",
        "def get_technical_indicators(data):\n",
        "    # Create 7 and 21 days Moving Average\n",
        "    data['MA7'] = data.iloc[:,4].rolling(window=7).mean()\n",
        "    data['MA21'] = data.iloc[:,4].rolling(window=21).mean()\n",
        "\n",
        "    # Create MACD\n",
        "    data['MACD'] = data.iloc[:,4].ewm(span=26).mean() - data.iloc[:,1].ewm(span=12,adjust=False).mean()\n",
        "\n",
        "    # Create Bollinger Bands\n",
        "    data['20SD'] = data.iloc[:, 4].rolling(20).std()\n",
        "    data['upper_band'] = data['MA21'] + (data['20SD'] * 2)\n",
        "    data['lower_band'] = data['MA21'] - (data['20SD'] * 2)\n",
        "\n",
        "    # Create Exponential moving average\n",
        "    data['EMA'] = data.iloc[:,4].ewm(com=0.5).mean()\n",
        "\n",
        "    # Create LogMomentum\n",
        "    data['logmomentum'] = np.log(data.iloc[:,4] - 1)\n",
        "    return data\n",
        "\n",
        "T_df = get_technical_indicators(df)\n",
        "\n",
        "#Drop the first 21 rows\n",
        "#For doing the fourier\n",
        "dataset = T_df.iloc[20:,:].reset_index(drop=True)\n",
        "\n",
        "#Getting the Fourier transform features\n",
        "def get_fourier_transfer(dataset):\n",
        "    # Get the columns for doing fourier\n",
        "    data_FT = dataset[['Date', 'Close']]\n",
        "\n",
        "    close_fft = np.fft.fft(np.asarray(data_FT['Close'].tolist()))\n",
        "    fft_df = pd.DataFrame({'fft': close_fft})\n",
        "    fft_df['absolute'] = fft_df['fft'].apply(lambda x: np.abs(x))\n",
        "    fft_df['angle'] = fft_df['fft'].apply(lambda x: np.angle(x))\n",
        "\n",
        "    fft_list = np.asarray(fft_df['fft'].tolist())\n",
        "    fft_com_df = pd.DataFrame()\n",
        "    for num_ in [3, 6, 9]:\n",
        "        fft_list_m10 = np.copy(fft_list);\n",
        "        fft_list_m10[num_:-num_] = 0\n",
        "        fft_ = np.fft.ifft(fft_list_m10)\n",
        "        fft_com = pd.DataFrame({'fft': fft_})\n",
        "        fft_com['absolute of ' + str(num_) + ' comp'] = fft_com['fft'].apply(lambda x: np.abs(x))\n",
        "        fft_com['angle of ' + str(num_) + ' comp'] = fft_com['fft'].apply(lambda x: np.angle(x))\n",
        "        fft_com = fft_com.drop(columns='fft')\n",
        "        fft_com_df = pd.concat([fft_com_df, fft_com], axis=1)\n",
        "\n",
        "    return fft_com_df\n",
        "\n",
        "#Get Fourier features\n",
        "dataset_F = get_fourier_transfer(dataset)\n",
        "Final_data = pd.concat([dataset, dataset_F], axis=1)\n",
        "\n",
        "sales_data_gan.to_csv(\"Finaldata_with_Fourier.csv\", index=False)\n",
        "\n",
        "def plot_technical_indicators(dataset, last_days):\n",
        "       plt.figure(figsize=(16, 10), dpi=100)\n",
        "       shape_0 = dataset.shape[0]\n",
        "       xmacd_ = shape_0 - last_days\n",
        "\n",
        "       dataset = dataset.iloc[-last_days:, :]\n",
        "       x_ = range(3, dataset.shape[0])\n",
        "       x_ = list(dataset.index)\n",
        "\n",
        "       # Plot first subplot\n",
        "       plt.subplot(2, 1, 1)\n",
        "       plt.plot(dataset['MA7'], label='MA 7', color='g', linestyle='--')\n",
        "       plt.plot(dataset['Close'], label='Closing Price', color='b')\n",
        "       plt.plot(dataset['MA21'], label='MA 21', color='r', linestyle='--')\n",
        "       plt.plot(dataset['upper_band'], label='Upper Band', color='c')\n",
        "       plt.plot(dataset['lower_band'], label='Lower Band', color='c')\n",
        "       plt.fill_between(x_, dataset['lower_band'], dataset['upper_band'], alpha=0.35)\n",
        "       plt.title('Technical indicators for Ford - last {} days.'.format(last_days))\n",
        "       plt.ylabel('USD')\n",
        "       plt.legend()\n",
        "\n",
        "       # Plot second subplot\n",
        "       plt.subplot(2, 1, 2)\n",
        "       plt.title('MACD')\n",
        "       plt.plot(dataset['MACD'], label='MACD', linestyle='-.')\n",
        "       plt.hlines(15, xmacd_, shape_0, colors='g', linestyles='--')\n",
        "       plt.hlines(-15, xmacd_, shape_0, colors='g', linestyles='--')\n",
        "       plt.plot(dataset['logmomentum'], label='Momentum', color='b', linestyle='-')\n",
        "       \n",
        "       plt.legend()\n",
        "       plt.show()\n",
        "\n",
        "plot_technical_indicators(T_df, 400)\n",
        "\n",
        "\n",
        "def plot_Fourier(dataset):\n",
        "    data_FT = dataset[['Date', 'Close']]\n",
        "\n",
        "    close_fft = np.fft.fft(np.asarray(data_FT['Close'].tolist()))\n",
        "    fft_df = pd.DataFrame({'fft': close_fft})\n",
        "    fft_df['absolute'] = fft_df['fft'].apply(lambda x: np.abs(x))\n",
        "    fft_df['angle'] = fft_df['fft'].apply(lambda x: np.angle(x))\n",
        "\n",
        "    fft_list = np.asarray(fft_df['fft'].tolist())\n",
        "    plt.figure(figsize=(14, 7), dpi=100)\n",
        "    fft_list = np.asarray(fft_df['fft'].tolist())\n",
        "    for num_ in [3, 6, 9]:\n",
        "        fft_list_m10 = np.copy(fft_list);\n",
        "        fft_list_m10[num_:-num_] = 0\n",
        "        plt.plot(np.fft.ifft(fft_list_m10), label='Fourier transform with {} components'.format(num_))\n",
        "    plt.plot(data_FT['Close'], label='Real')\n",
        "    plt.xlabel('Days')\n",
        "    plt.ylabel('USD')\n",
        "    plt.title('Ford (close) stock prices & Fourier transforms')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_Fourier(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMmQZb3AzBxy"
      },
      "source": [
        "### **2.1.2: Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGHRn39HzDui"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from numpy import *\n",
        "from math import sqrt\n",
        "from pandas import *\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from pickle import dump\n",
        "\n",
        "# %% --------------------------------------- Load Data  -----------------------------------------------------------------\n",
        "dataset = pd.read_csv('Finaldata_with_Fourier.csv', parse_dates=['Date'])\n",
        "\n",
        "# %% --------------------------------------- Data Preprocessing  -----------------------------------------------------------------\n",
        "\n",
        "# Replace 0 by NA\n",
        "dataset.replace(0, np.nan, inplace=True)\n",
        "dataset.to_csv(\"dataset.csv\", index=False)\n",
        "# Add News data\n",
        "# dataset[\"News\"] = news[\"Score\"]\n",
        "\n",
        "# Check NA and fill them\n",
        "dataset.isnull().sum()\n",
        "dataset.iloc[:, 1:] = pd.concat([dataset.iloc[:, 1:].ffill(), dataset.iloc[:, 1:].bfill()]).groupby(level=0).mean()\n",
        "print(dataset.columns)\n",
        "\n",
        "# Set the date to datetime data\n",
        "datetime_series = pd.to_datetime(dataset['Date'])\n",
        "datetime_index = pd.DatetimeIndex(datetime_series.values)\n",
        "dataset = dataset.set_index(datetime_index)\n",
        "dataset = dataset.sort_values(by='Date')\n",
        "dataset = dataset.drop(columns='Date')\n",
        "\n",
        "# Get features and target\n",
        "X_value = pd.DataFrame(dataset.iloc[:, 1:])\n",
        "y_value = pd.DataFrame(dataset.iloc[:, 0])\n",
        "\n",
        "# Autocorrelation Check\n",
        "sm.graphics.tsa.plot_acf(y_value.squeeze(), lags=100)\n",
        "plt.show()\n",
        "\n",
        "# Normalized the data\n",
        "X_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "y_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "X_scaler.fit(X_value)\n",
        "y_scaler.fit(y_value)\n",
        "\n",
        "X_scale_dataset = X_scaler.fit_transform(X_value)\n",
        "y_scale_dataset = y_scaler.fit_transform(y_value)\n",
        "\n",
        "dump(X_scaler, open('X_scaler.pkl', 'wb'))\n",
        "dump(y_scaler, open('y_scaler.pkl', 'wb'))\n",
        "\n",
        "# Reshape the data\n",
        "'''Set the data input steps and output steps, \n",
        "    we use 30 days data to predict 1 day price here, \n",
        "    reshape it to (None, input_step, number of features) used for LSTM input'''\n",
        "n_steps_in = 3\n",
        "n_features = X_value.shape[1]\n",
        "n_steps_out = 1\n",
        "\n",
        "# Get X/y dataset\n",
        "def get_X_y(X_data, y_data):\n",
        "    X = list()\n",
        "    y = list()\n",
        "    yc = list()\n",
        "\n",
        "    length = len(X_data)\n",
        "    for i in range(0, length, 1):\n",
        "        X_value = X_data[i: i + n_steps_in][:, :]\n",
        "        y_value = y_data[i + n_steps_in: i + (n_steps_in + n_steps_out)][:, 0]\n",
        "        yc_value = y_data[i: i + n_steps_in][:, :]\n",
        "        if len(X_value) == 3 and len(y_value) == 1:\n",
        "            X.append(X_value)\n",
        "            y.append(y_value)\n",
        "            yc.append(yc_value)\n",
        "\n",
        "    return np.array(X), np.array(y), np.array(yc)\n",
        "\n",
        "# get the train test predict index\n",
        "def predict_index(dataset, X_train, n_steps_in, n_steps_out):\n",
        "\n",
        "    # get the predict data (remove the in_steps days)\n",
        "    train_predict_index = dataset.iloc[n_steps_in : X_train.shape[0] + n_steps_in + n_steps_out - 1, :].index\n",
        "    test_predict_index = dataset.iloc[X_train.shape[0] + n_steps_in:, :].index\n",
        "\n",
        "    return train_predict_index, test_predict_index\n",
        "\n",
        "# Split train/test dataset\n",
        "def split_train_test(data):\n",
        "    train_size = round(len(X) * 0.94)\n",
        "    data_train = data[0:train_size]\n",
        "    data_test = data[train_size:]\n",
        "    return data_train, data_test\n",
        "\n",
        "# Get data and check shape\n",
        "X, y, yc = get_X_y(X_scale_dataset, y_scale_dataset)\n",
        "X_train, X_test, = split_train_test(X)\n",
        "y_train, y_test, = split_train_test(y)\n",
        "yc_train, yc_test, = split_train_test(yc)\n",
        "index_train, index_test, = predict_index(dataset, X_train, n_steps_in, n_steps_out)\n",
        "# %% --------------------------------------- Save dataset -----------------------------------------------------------------\n",
        "print('X shape: ', X.shape)\n",
        "print('y shape: ', y.shape)\n",
        "print('X_train shape: ', X_train.shape)\n",
        "print('y_train shape: ', y_train.shape)\n",
        "print('y_c_train shape: ', yc_train.shape)\n",
        "print('X_test shape: ', X_test.shape)\n",
        "print('y_test shape: ', y_test.shape)\n",
        "print('y_c_test shape: ', yc_test.shape)\n",
        "print('index_train shape:', index_train.shape)\n",
        "print('index_test shape:', index_test.shape)\n",
        "\n",
        "np.save(\"X_train.npy\", X_train)\n",
        "np.save(\"y_train.npy\", y_train)\n",
        "np.save(\"X_test.npy\", X_test)\n",
        "np.save(\"y_test.npy\", y_test)\n",
        "np.save(\"yc_train.npy\", yc_train)\n",
        "np.save(\"yc_test.npy\", yc_test)\n",
        "np.save('index_train.npy', index_train)\n",
        "np.save('index_test.npy', index_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN_Yn-_dzJ_C"
      },
      "source": [
        "### **2.1.3: WGAN_GP**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.8.0"
      ],
      "metadata": {
        "id": "HVJ4SfhUWI1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf5nt3SFz_sP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from pickle import load\n",
        "from tensorflow.keras.losses import mean_squared_error\n",
        "from tensorflow.keras.layers import GRU, Dense, Flatten, Conv1D, BatchNormalization, LeakyReLU, ELU, ReLU\n",
        "from tensorflow.keras import Sequential, regularizers\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "# Load data\n",
        "X_train = np.load(\"X_train.npy\", allow_pickle=True)\n",
        "y_train = np.load(\"y_train.npy\", allow_pickle=True)\n",
        "X_test = np.load(\"X_test.npy\", allow_pickle=True)\n",
        "y_test = np.load(\"y_test.npy\", allow_pickle=True)\n",
        "yc_train = np.load(\"yc_train.npy\", allow_pickle=True)\n",
        "yc_test = np.load(\"yc_test.npy\", allow_pickle=True)\n",
        "\n",
        "# Define the generator\n",
        "def Generator(input_dim, output_dim, feature_size) -> tf.keras.models.Model:\n",
        "    model = Sequential()\n",
        "    model.add(GRU(units=1024,\n",
        "                  return_sequences=True,\n",
        "                  input_shape=(input_dim, feature_size),\n",
        "                  recurrent_dropout=0.02,\n",
        "                  recurrent_regularizer=regularizers.l2(1e-3)))\n",
        "    model.add(GRU(units=512,\n",
        "                  return_sequences=True,\n",
        "                  recurrent_dropout=0.02,\n",
        "                  recurrent_regularizer=regularizers.l2(1e-3)))\n",
        "    model.add(GRU(units=256,\n",
        "                  recurrent_dropout=0.02,\n",
        "                  recurrent_regularizer=regularizers.l2(1e-3)))\n",
        "    model.add(Dense(128, kernel_regularizer=regularizers.l2(1e-3)))\n",
        "    model.add(Dense(64, kernel_regularizer=regularizers.l2(1e-3)))\n",
        "    model.add(Dense(units=output_dim))\n",
        "    return model\n",
        "\n",
        "# Define the discriminator\n",
        "def Discriminator() -> tf.keras.models.Model:\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Conv1D(32, input_shape=(4, 1), kernel_size=3, strides=2, padding=\"same\", activation=LeakyReLU(alpha=0.01)))\n",
        "    model.add(Conv1D(64, kernel_size=3, strides=2, padding=\"same\", activation=LeakyReLU(alpha=0.01)))\n",
        "    model.add(Conv1D(128, kernel_size=3, strides=2, padding=\"same\", activation=LeakyReLU(alpha=0.01)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(220, use_bias=True))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(Dense(220, use_bias=True))\n",
        "    model.add(ReLU())\n",
        "    model.add(Dense(1))\n",
        "    return model\n",
        "\n",
        "# Train WGAN-GP model\n",
        "class GAN():\n",
        "    def __init__(self, generator, discriminator):\n",
        "        super(GAN, self).__init__()\n",
        "        self.d_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
        "        self.g_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.batch_size = 128\n",
        "        checkpoint_dir = '../training_checkpoints'\n",
        "        self.checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "        self.checkpoint = tf.train.Checkpoint(generator_optimizer=self.g_optimizer,\n",
        "                                              discriminator_optimizer=self.d_optimizer,\n",
        "                                              generator=self.generator,\n",
        "                                              discriminator=self.discriminator)\n",
        "\n",
        "    def gradient_penalty(self, batch_size, real_output, fake_output):\n",
        "        \"\"\" Calculates the gradient penalty.\n",
        "\n",
        "        This loss is calculated on an interpolated image\n",
        "        and added to the discriminator loss.\n",
        "        \"\"\"\n",
        "        # get the interpolated data\n",
        "        alpha = tf.random.normal([batch_size, 4, 1], 0.0, 1.0)\n",
        "        diff = fake_output - tf.cast(real_output, tf.float32)\n",
        "        interpolated = tf.cast(real_output, tf.float32) + alpha * diff\n",
        "\n",
        "        with tf.GradientTape() as gp_tape:\n",
        "            gp_tape.watch(interpolated)\n",
        "            # 1. Get the discriminator output for this interpolated image.\n",
        "            pred = self.discriminator(interpolated, training=True)\n",
        "            \n",
        "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
        "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "\n",
        "        # 3. Calcuate the norm of the gradients\n",
        "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]))\n",
        "\n",
        "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "        return gp\n",
        "\n",
        "    def train_step(self, data):\n",
        "        real_input, real_price, yc = data\n",
        "        batch_size = tf.shape(real_input)[0]\n",
        "        for _ in range(1):\n",
        "            with tf.GradientTape() as d_tape:\n",
        "                # Train the discriminator\n",
        "                # generate fake output\n",
        "                generated_data = self.generator(real_input, training=True)\n",
        "                # reshape the data\n",
        "                generated_data_reshape = tf.reshape(generated_data, [generated_data.shape[0], generated_data.shape[1], 1])\n",
        "                fake_output = tf.concat([generated_data_reshape, tf.cast(yc, tf.float32)], axis=1)\n",
        "                real_y_reshape = tf.reshape(real_price, [real_price.shape[0], real_price.shape[1], 1])\n",
        "                real_output = tf.concat([tf.cast(real_y_reshape, tf.float32), tf.cast(yc, tf.float32)], axis=1)\n",
        "                # Get the logits for the fake images\n",
        "                D_real = self.discriminator(real_output, training=True)\n",
        "                # Get the logits for real images\n",
        "                D_fake = self.discriminator(fake_output, training=True)\n",
        "                # Calculate discriminator loss using fake and real logits\n",
        "                real_loss = tf.cast(tf.reduce_mean(D_real), tf.float32)\n",
        "                fake_loss = tf.cast(tf.reduce_mean(D_fake), tf.float32)\n",
        "                d_cost = fake_loss-real_loss\n",
        "                # Calculate the gradientjiu penalty\n",
        "                gp = self.gradient_penalty(batch_size, real_output, fake_output)\n",
        "                # Add the gradient penalty to the original discriminator loss\n",
        "                d_loss = d_cost + gp * 10\n",
        "\n",
        "            d_grads = d_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "            self.d_optimizer.apply_gradients(zip(d_grads, self.discriminator.trainable_variables))\n",
        "        for _ in range(3):\n",
        "            with tf.GradientTape() as g_tape:\n",
        "                # Train the generator\n",
        "                # generate fake output\n",
        "                generated_data = self.generator(real_input, training=True)\n",
        "                # reshape the data\n",
        "                generated_data_reshape = tf.reshape(generated_data, [generated_data.shape[0], generated_data.shape[1], 1])\n",
        "                fake_output = tf.concat([generated_data_reshape, tf.cast(yc, tf.float32)], axis=1)\n",
        "                # Get the discriminator logits for fake images\n",
        "                G_fake = self.discriminator(fake_output, training=True)\n",
        "                # Calculate the generator loss\n",
        "                g_loss = -tf.reduce_mean(G_fake)\n",
        "            g_grads = g_tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "            self.g_optimizer.apply_gradients(zip(g_grads, self.generator.trainable_variables))\n",
        "\n",
        "        return real_price, generated_data, {'d_loss': d_loss, 'g_loss': g_loss}\n",
        "\n",
        "    def train(self, X_train, y_train, yc, epochs):\n",
        "        data = X_train, y_train, yc\n",
        "        train_hist = {}\n",
        "        train_hist['D_losses'] = []\n",
        "        train_hist['G_losses'] = []\n",
        "        train_hist['per_epoch_times'] = []\n",
        "        train_hist['total_ptime'] = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            start = time.time()\n",
        "\n",
        "            real_price, fake_price, loss = self.train_step(data)\n",
        "\n",
        "            G_losses = []\n",
        "            D_losses = []\n",
        "\n",
        "            Real_price = []\n",
        "            Predicted_price = []\n",
        "\n",
        "            D_losses.append(loss['d_loss'].numpy())\n",
        "            G_losses.append(loss['g_loss'].numpy())\n",
        "\n",
        "            Predicted_price.append(fake_price)\n",
        "            Real_price.append(real_price)\n",
        "\n",
        "            # Save the model every 15 epochs\n",
        "            if (epoch + 1) % 15 == 0:\n",
        "                tf.keras.models.save_model(generator, 'gen_GRU_model_%d.h5' % epoch)\n",
        "                self.checkpoint.save(file_prefix=self.checkpoint_prefix)\n",
        "                print('epoch', epoch+1, 'd_loss', loss['d_loss'].numpy(), 'g_loss', loss['g_loss'].numpy())\n",
        "\n",
        "            # For printing loss\n",
        "            epoch_end_time = time.time()\n",
        "            per_epoch_ptime = epoch_end_time - start\n",
        "            train_hist['D_losses'].append(D_losses)\n",
        "            train_hist['G_losses'].append(G_losses)\n",
        "            train_hist['per_epoch_times'].append(per_epoch_ptime)\n",
        "            \n",
        "        # Reshape the predicted result & real\n",
        "        Predicted_price = np.array(Predicted_price)\n",
        "        Predicted_price = Predicted_price.reshape(Predicted_price.shape[1], Predicted_price.shape[2])\n",
        "        Real_price = np.array(Real_price)\n",
        "        Real_price = Real_price.reshape(Real_price.shape[1], Real_price.shape[2])\n",
        "\n",
        "        # Plot the loss\n",
        "        plt.plot(train_hist['D_losses'], label='D_loss')\n",
        "        plt.plot(train_hist['G_losses'], label='G_loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        plt.savefig('train_loss.png')\n",
        "\n",
        "        print(\"REAL\", Real_price.shape)\n",
        "        print(Real_price)\n",
        "        print(\"PREDICTED\", Predicted_price.shape)\n",
        "        print(Predicted_price)\n",
        "\n",
        "        return Predicted_price, Real_price, np.sqrt(mean_squared_error(Real_price, Predicted_price)) / np.mean(Real_price)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    input_dim = X_train.shape[1]\n",
        "    feature_size = X_train.shape[2]\n",
        "    output_dim = y_train.shape[1]\n",
        "    epoch = 1000\n",
        "\n",
        "    generator = Generator(X_train.shape[1], output_dim, X_train.shape[2])\n",
        "    discriminator = Discriminator()\n",
        "    gan = GAN(generator, discriminator)\n",
        "    Predicted_price, Real_price, RMSPE = gan.train(X_train, y_train, yc_train, epoch)\n",
        "\n",
        "# %% --------------------------------------- Plot the result -----------------------------------------------------\n",
        "\n",
        "# Rescale back the real dataset\n",
        "X_scaler = load(open('X_scaler.pkl', 'rb'))\n",
        "y_scaler = load(open('y_scaler.pkl', 'rb'))\n",
        "train_predict_index = np.load(\"index_train.npy\", allow_pickle=True)\n",
        "test_predict_index = np.load(\"index_test.npy\", allow_pickle=True)\n",
        "\n",
        "print(\"----- predicted price -----\", Predicted_price)\n",
        "\n",
        "rescaled_Real_price = y_scaler.inverse_transform(Real_price)\n",
        "rescaled_Predicted_price = y_scaler.inverse_transform(Predicted_price)\n",
        "\n",
        "print(\"----- rescaled predicted price -----\", rescaled_Predicted_price)\n",
        "print(\"----- SHAPE rescaled predicted price -----\", rescaled_Predicted_price.shape)\n",
        "\n",
        "predict_result = pd.DataFrame()\n",
        "for i in range(rescaled_Predicted_price.shape[0]):\n",
        "    y_predict = pd.DataFrame(rescaled_Predicted_price[i], columns=[\"predicted_price\"], index=train_predict_index[i:i+output_dim])\n",
        "    predict_result = pd.concat([predict_result, y_predict], axis=1, sort=False)\n",
        "\n",
        "real_price = pd.DataFrame()\n",
        "for i in range(rescaled_Real_price.shape[0]):\n",
        "    y_train = pd.DataFrame(rescaled_Real_price[i], columns=[\"real_price\"], index=train_predict_index[i:i+output_dim])\n",
        "    real_price = pd.concat([real_price, y_train], axis=1, sort=False)\n",
        "\n",
        "predict_result['predicted_mean'] = predict_result.mean(axis=1)\n",
        "real_price['real_mean'] = real_price.mean(axis=1)\n",
        "\n",
        "# Plot the predicted result\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.plot(real_price[\"real_mean\"])\n",
        "plt.plot(predict_result[\"predicted_mean\"], color = 'r')\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Stock price\")\n",
        "plt.legend((\"Real price\", \"Predicted price\"), loc=\"upper left\", fontsize=16)\n",
        "plt.title(\"The result of Training\", fontsize=20)\n",
        "plt.show()\n",
        "plt.savefig('train_plot.png')\n",
        "\n",
        "# Calculate RMSE\n",
        "predicted = predict_result[\"predicted_mean\"]\n",
        "real = real_price[\"real_mean\"]\n",
        "For_MSE = pd.concat([predicted, real], axis = 1)\n",
        "RMSE = np.sqrt(mean_squared_error(predicted, real))\n",
        "print('-- RMSE -- ', RMSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeHCn8Fg0E7g"
      },
      "source": [
        "### **2.1.4: Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPFxByce0CGQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from pickle import load\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "########### Test dataset #########\n",
        "\n",
        "# Load scaler/ index\n",
        "X_scaler = load(open('X_scaler.pkl', 'rb'))\n",
        "y_scaler = load(open('y_scaler.pkl', 'rb'))\n",
        "train_predict_index = np.load(\"index_train.npy\", allow_pickle=True)\n",
        "test_predict_index = np.load(\"index_test.npy\", allow_pickle=True)\n",
        "\n",
        "# Load test dataset/ model\n",
        "G_model = tf.keras.models.load_model('gen_GRU_model_989.h5')\n",
        "X_test = np.load(\"X_test.npy\", allow_pickle=True)\n",
        "y_test = np.load(\"y_test.npy\", allow_pickle=True)\n",
        "\n",
        "\n",
        "def get_test_plot(X_test, y_test):\n",
        "    # Set output steps\n",
        "    output_dim = y_test.shape[1]\n",
        "\n",
        "    # Get predicted data\n",
        "    y_predicted = G_model(X_test)\n",
        "    rescaled_real_y = y_scaler.inverse_transform(y_test)\n",
        "    rescaled_predicted_y = y_scaler.inverse_transform(y_predicted)\n",
        "\n",
        "    ## Predicted price\n",
        "    predict_result = pd.DataFrame()\n",
        "    for i in range(rescaled_predicted_y.shape[0]):\n",
        "        y_predict = pd.DataFrame(rescaled_predicted_y[i], columns=[\"predicted_price\"],\n",
        "                                 index=test_predict_index[i:i + output_dim])\n",
        "        predict_result = pd.concat([predict_result, y_predict], axis=1, sort=False)\n",
        "\n",
        "    ## Real price\n",
        "    real_price = pd.DataFrame()\n",
        "    for i in range(rescaled_real_y.shape[0]):\n",
        "        y_train = pd.DataFrame(rescaled_real_y[i], columns=[\"real_price\"], index=test_predict_index[i:i + output_dim])\n",
        "        real_price = pd.concat([real_price, y_train], axis=1, sort=False)\n",
        "\n",
        "    predict_result['predicted_mean'] = predict_result.mean(axis=1)\n",
        "    real_price['real_mean'] = real_price.mean(axis=1)\n",
        "\n",
        "    #drop 2020\n",
        "    # Input_Before = '2020-01-01'\n",
        "    # predict_result = predict_result.loc[predict_result.index < Input_Before]\n",
        "    # real_price = real_price.loc[real_price.index < Input_Before]\n",
        "\n",
        "    # Plot the predicted result\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    plt.plot(real_price[\"real_mean\"])\n",
        "    plt.plot(predict_result[\"predicted_mean\"], color='r')\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Stock price\")\n",
        "    plt.legend((\"Real price\", \"Predicted price\"), loc=\"upper left\", fontsize=16)\n",
        "    plt.title(\"The result of test\", fontsize=20)\n",
        "    plt.show()\n",
        "    plt.savefig('test_plot.png')\n",
        "    # Calculate RMSE\n",
        "    predicted = predict_result[\"predicted_mean\"]\n",
        "    real = real_price[\"real_mean\"]\n",
        "    For_MSE = pd.concat([predicted, real], axis=1)\n",
        "    RMSE = np.sqrt(mean_squared_error(predicted, real))\n",
        "    print('-- RMSE -- ', RMSE)\n",
        "\n",
        "    return predict_result, RMSE\n",
        "\n",
        "test_predicted, test_RMSE = get_test_plot(X_test, y_test)\n",
        "test_predicted.to_csv(\"test_predicted.csv\")\n",
        "\n",
        "# ######### Test dataset #########\n",
        "# ##### For last set #########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4lv4TFZRxzI"
      },
      "source": [
        "## **2.2: Deep Reinforcement Learning Prediction Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU_TTlWbM0Aw"
      },
      "source": [
        "### **2.2.1: RDPG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp0hD3LMu8zd"
      },
      "outputs": [],
      "source": [
        "%pip install tensorflow==1.15.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veThKLLqt8l4"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This part of code is to load and preprocess time series data.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "def build_s_a(sequence,n,m):\n",
        "    '''\n",
        "    Args:\n",
        "        sequence: Time series data\n",
        "        n: The number of historical data denoting the current state\n",
        "        m: The number of prediction steps in advance\n",
        "    Return:\n",
        "        state_mat: A matrix contains all states at each time step\n",
        "        best_action: The optimal action based on each state\n",
        "    '''\n",
        "    n_rows = len(sequence)-n-m+1\n",
        "    state_mat = np.zeros((n_rows,n))\n",
        "    best_action = np.zeros(n_rows)\n",
        "    for i in range(n_rows):\n",
        "        state_mat[i] = sequence[i:(i+n)]\n",
        "        best_action[i] = sequence[i+n+m-1]\n",
        "    return state_mat,best_action\n",
        "\n",
        "def normalization(traindata,testdata):\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(traindata)\n",
        "    traindata_scaled = scaler.transform(traindata)\n",
        "    testdata_scaled = scaler.transform(testdata)\n",
        "    \n",
        "    return traindata_scaled,testdata_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EyT4fmgM3XQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The agent of RL algorithm Recurrent Detrministic Policy Gradient.\n",
        "The Actor NNs are deployed as three-layer Fully-Connected NN.\n",
        "The Critic NNs are deployed as RNN.\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class RDPG():\n",
        "    def __init__(self,\n",
        "                 n_features,\n",
        "                 a_low,\n",
        "                 a_high,\n",
        "                 learning_rate_actor,\n",
        "                 learning_rate_critic,\n",
        "                 n_actor_hidden,\n",
        "                 n_critic_hidden,\n",
        "                 gamma = 0.9,\n",
        "                 noise_varience = 3,\n",
        "                 soft_replace = 0.1,\n",
        "                 memory_size = 1000,\n",
        "                 batch_size = 128):\n",
        "        self.n_features = n_features             #dimension of states      \n",
        "        self.a_low = a_low                       #The low bound of action sapce\n",
        "        self.a_high = a_high                     #The high bound of action space\n",
        "        self.lr_a = learning_rate_actor          #Learning rate of Actor NN\n",
        "        self.lr_c = learning_rate_critic         #Learning rate of Critic NN\n",
        "        self.n_actor_hidden = n_actor_hidden     #Number of hidden layer neurons in Actor\n",
        "        self.n_critic_cells = n_critic_hidden    #Number of hidden layer neurons in Critic\n",
        "        self.gamma = gamma                       #Reward discount rate\n",
        "        self.noise_var = noise_varience          #Variance of output action distribution\n",
        "        self.soft_replace = soft_replace         #Update speed of target networks\n",
        "        self.memory_size = memory_size           #Size of experience replay buffer\n",
        "        self.memory = deque(maxlen = self.memory_size)   #Experience replay buffer\n",
        "        self.batch_size = batch_size                     \n",
        "        \n",
        "        self.s = tf.placeholder(dtype = tf.float32,shape = [None,self.n_features])\n",
        "        self.s_ = tf.placeholder(dtype = tf.float32,shape = [None,self.n_features])\n",
        "        self.r = tf.placeholder(dtype = tf.float32,shape = [None,])\n",
        "        self.done = tf.placeholder(dtype = tf.float32,shape = [None,]) # 0 if s_ == terminal else 1\n",
        "        \n",
        "        self.a = self.build_Actor1()\n",
        "        self.a_ = self.build_Actor2()\n",
        "        self.q_sa = self.build_Critic1()      #shape:[None,] \n",
        "        self.q_s_a_ = self.build_Critic2()    #shape:[None,]\n",
        "        \n",
        "        self.curr_a_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                                            scope = 'Actor/Current')\n",
        "        self.targ_a_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                                            scope = 'Actor/Target')\n",
        "        self.curr_c_params= tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                                            scope = 'Critic/Current')\n",
        "        self.targ_c_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                                            scope = 'Critic/Target')\n",
        "        \n",
        "        # Soft replace of Targets NN parameters\n",
        "        self.replace_a_params = [tf.assign(t,(1-self.soft_replace)*t + self.soft_replace*e) \\\n",
        "                                 for (t,e) in zip(self.targ_a_params,self.curr_a_params)]\n",
        "        self.replace_c_params = [tf.assign(t,(1-self.soft_replace)*t + self.soft_replace*e) \\\n",
        "                                 for (t,e) in zip(self.targ_c_params,self.curr_c_params)]\n",
        "        \n",
        "        self.td_error = self.r + self.gamma*self.q_s_a_ - self.q_sa\n",
        "        self.critic_loss = tf.reduce_mean(tf.square(self.td_error))\n",
        "        self.actor_loss = -tf.reduce_mean(self.q_sa)\n",
        "        \n",
        "        self.actor_train_op = tf.train.AdamOptimizer(self.lr_a).minimize(self.actor_loss,\n",
        "                                                    var_list = self.curr_a_params)\n",
        "        self.critic_train_op = tf.train.AdamOptimizer(self.lr_c).minimize(self.critic_loss,\n",
        "                                                     var_list = self.curr_c_params)\n",
        "        \n",
        "        self.learn_step_counter = 0\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    def build_Actor1(self):\n",
        "        '''\n",
        "        Building Current Actor network.\n",
        "        '''\n",
        "        with tf.variable_scope('Actor/Current'):\n",
        "            w_init = tf.random_normal_initializer(0,0.1)\n",
        "            b_init = tf.constant_initializer(0.1)\n",
        "            w1 = tf.get_variable(name = 'w1',shape = [self.n_features,self.n_actor_hidden],\n",
        "                                 dtype = tf.float32,initializer = w_init,\n",
        "                                 trainable = True)\n",
        "            b1 = tf.get_variable('b1',shape = [self.n_actor_hidden,],\n",
        "                                 dtype = tf.float32,initializer = b_init,\n",
        "                                 trainable = True)\n",
        "            w2 = tf.get_variable('w2',shape = [self.n_actor_hidden,1],\n",
        "                                 dtype = tf.float32,initializer = w_init,\n",
        "                                 trainable = True)\n",
        "            b2 = tf.get_variable('b2',shape = [1,],\n",
        "                                 dtype = tf.float32,initializer = b_init,\n",
        "                                 trainable = True)\n",
        "            hidden = tf.nn.relu(tf.matmul(self.s,w1) + b1)\n",
        "            a = tf.matmul(hidden,w2) + b2\n",
        "        return a[:,0]\n",
        "    \n",
        "    def build_Actor2(self):\n",
        "        '''\n",
        "        Building Target Actor network.\n",
        "        '''\n",
        "        with tf.variable_scope('Actor/Target'):\n",
        "            w_init = tf.random_normal_initializer(0,0.1)\n",
        "            b_init = tf.constant_initializer(0.1)\n",
        "            w1 = tf.get_variable('w1',shape = [self.n_features,self.n_actor_hidden],\n",
        "                                 dtype = tf.float32,initializer = w_init,\n",
        "                                 trainable = False)\n",
        "            b1 = tf.get_variable('b1',shape = [self.n_actor_hidden,],\n",
        "                                 dtype = tf.float32,initializer = b_init,\n",
        "                                 trainable = False)\n",
        "            w2 = tf.get_variable('w2',shape = [self.n_actor_hidden,1],\n",
        "                                 dtype = tf.float32,initializer = w_init,\n",
        "                                 trainable = False)\n",
        "            b2 = tf.get_variable('b2',shape = [1,],\n",
        "                                 dtype = tf.float32,initializer = b_init,\n",
        "                                 trainable = False)\n",
        "            hidden = tf.nn.relu(tf.matmul(self.s_,w1) + b1)\n",
        "            a_ = tf.matmul(hidden,w2) + b2\n",
        "        return a_[:,0]\n",
        "    \n",
        "    def build_Critic1(self):\n",
        "        '''\n",
        "        Building Current Critic network.\n",
        "        '''\n",
        "        with tf.variable_scope('Critic/Current'):\n",
        "            w_init = tf.random_normal_initializer(0,0.1)\n",
        "            b_init = tf.constant_initializer(0.1)\n",
        "            \n",
        "            rnn_cell = tf.contrib.rnn.BasicRNNCell(self.n_critic_cells)\n",
        "            self.init_state = rnn_cell.zero_state(batch_size=1, dtype=tf.float64)\n",
        "            s = tf.cast(tf.expand_dims(self.s,axis = 1),tf.float64)\n",
        "            \n",
        "            outputs, self.final_state = tf.nn.dynamic_rnn(\n",
        "                    cell = rnn_cell, inputs = s, \n",
        "                    initial_state = self.init_state, time_major = True)\n",
        "            cell_out = tf.cast(tf.reshape(outputs, [-1, self.n_critic_cells]),tf.float32)\n",
        "            \n",
        "            a_out = tf.layers.dense(self.a[:,np.newaxis],self.n_critic_cells,trainable = True)\n",
        "            q_sa = tf.layers.dense(cell_out + a_out,1,tf.nn.relu,\n",
        "                                   kernel_initializer = w_init,\n",
        "                                   bias_initializer = b_init,trainable = True)\n",
        "        return q_sa[:,0]\n",
        "\n",
        "    def build_Critic2(self):\n",
        "        '''\n",
        "        Building Target Critic network.\n",
        "        '''\n",
        "        with tf.variable_scope('Critic/Target'):\n",
        "            w_init = tf.random_normal_initializer(0,0.1)\n",
        "            b_init = tf.constant_initializer(0.1)\n",
        "            \n",
        "            rnn_cell = tf.contrib.rnn.BasicRNNCell(self.n_critic_cells)\n",
        "            self.init_state = rnn_cell.zero_state(batch_size=1, dtype=tf.float64)\n",
        "            s_ = tf.cast(tf.expand_dims(self.s_,axis = 1),tf.float64)\n",
        "            \n",
        "            outputs, self.final_state = tf.nn.dynamic_rnn(\n",
        "                    cell = rnn_cell, inputs = s_, \n",
        "                    initial_state = self.init_state, time_major = True)\n",
        "            cell_out = tf.cast(tf.reshape(outputs, [-1, self.n_critic_cells]),tf.float32)\n",
        "            \n",
        "            a_out = tf.layers.dense(self.a_[:,np.newaxis],self.n_critic_cells,trainable = False)\n",
        "            q_s_a_ = tf.layers.dense(cell_out + a_out,1,tf.nn.relu,\n",
        "                                   kernel_initializer = w_init,\n",
        "                                   bias_initializer = b_init,trainable = False)\n",
        "        return q_s_a_[:,0]         \n",
        "    \n",
        "    def choose_action(self,state):\n",
        "        state = np.reshape(state,[-1,self.n_features])\n",
        "        action = self.sess.run(self.a,feed_dict = {self.s:state})\n",
        "        return action\n",
        "    \n",
        "    def store_transition(self,state,action,reward,next_state):\n",
        "        state,next_state = state[np.newaxis,:],next_state[np.newaxis,:]\n",
        "        action,reward = np.array(action),np.array(reward)\n",
        "        action = np.reshape(action,[1,-1])\n",
        "        reward = np.reshape(reward,[1,-1])\n",
        "        \n",
        "        transition = np.concatenate((state,action,reward,next_state),axis = 1)\n",
        "        self.memory.append(transition[0,:])\n",
        "    \n",
        "    def learn(self):\n",
        "        if len(self.memory) == self.memory_size:\n",
        "            if self.learn_step_counter % 200 == 0:\n",
        "                self.sess.run((self.replace_a_params,self.replace_c_params))\n",
        "            \n",
        "            self.noise_var *= 0.999\n",
        "                \n",
        "            batch = np.array(random.sample(self.memory,self.batch_size))\n",
        "            batch_s = batch[:,:self.n_features]\n",
        "            batch_a = batch[:,self.n_features:(self.n_features + 1)][:,0]\n",
        "            batch_r = batch[:,(self.n_features + 1):(self.n_features + 2)][:,0]\n",
        "            batch_s_ = batch[:,(self.n_features + 2):(self.n_features*2 + 2)]\n",
        "            \n",
        "            self.sess.run(self.actor_train_op,feed_dict = {self.s:batch_s})\n",
        "            self.sess.run(self.critic_train_op,feed_dict = {self.s:batch_s,\n",
        "                                                            self.a:batch_a,\n",
        "                                                            self.s_:batch_s_,\n",
        "                                                            self.r:batch_r})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovFwUx0LuKW3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "#####################  hyper parameters  ####################\n",
        "N_FEATURES = 6\n",
        "A_LOW = 0\n",
        "A_HIGH = 1\n",
        "LR_A = 0.001\n",
        "LR_C = 0.003\n",
        "N_ACTOR_HIDDEN = 30\n",
        "N_CRITIC_HIDDEN = 30\n",
        "MAX_EPISODES = 200\n",
        "MAX_STEPS = 1000\n",
        "\n",
        "GAMMA = 0.9                # 折扣因子\n",
        "TAU = 0.1                 # 软更新因子\n",
        "MEMORY_CAPACITY = 100000    #记忆库大小\n",
        "BATCH_SIZE = 128            #批梯度下降的m\n",
        "#############################################################\n",
        "\n",
        "#Load data \n",
        "data = sales_data['TTM Net Sales']\n",
        "\n",
        "#Build state matrix and best action\n",
        "state,action = build_s_a(data,N_FEATURES, 15)\n",
        "\n",
        "#Data split\n",
        "SPLIT_RATE = 0.94\n",
        "split_index = round(len(state)*SPLIT_RATE)\n",
        "train_s,train_a = state[:split_index],action[:split_index]\n",
        "test_s,test_a = state[split_index:],action[split_index:]\n",
        "\n",
        "#Normalization\n",
        "train_s_scaled,test_s_scaled = normalization(train_s,test_s)\n",
        "A,B = train_a.max(),train_a.min()\n",
        "train_a_scaled,test_a_scaled = (train_a-B)/(A-B),(test_a-B)/(A-B)\n",
        "\n",
        "# Training\n",
        "rdpg = RDPG(N_FEATURES,A_LOW,A_HIGH,LR_A,LR_C,N_ACTOR_HIDDEN,N_CRITIC_HIDDEN)\n",
        "for episode  in range(MAX_EPISODES):\n",
        "    index = np.random.choice(range(len(train_s_scaled) - 1))\n",
        "    s = train_s_scaled[index]\n",
        "    ep_reward = 0\n",
        "    \n",
        "    for step in range(MAX_STEPS):\n",
        "        a = rdpg.choose_action(s)\n",
        "        r = -abs(a-train_a_scaled[index])\n",
        "        ep_reward += r\n",
        "        index += 1\n",
        "        s_ = train_s_scaled[index]     \n",
        "        rdpg.store_transition(s,a,r,s_)\n",
        "        rdpg.learn()\n",
        "        if (index == len(train_s_scaled)-1) or (step == MAX_STEPS-1):\n",
        "            print('Episode %d : %.2f'%(episode,ep_reward))\n",
        "            break \n",
        "        s = s_\n",
        "\n",
        "# Testing\n",
        "pred = []\n",
        "for i in range(len(test_s_scaled)):\n",
        "    state = test_s_scaled[i]\n",
        "    action = rdpg.choose_action(state)\n",
        "    pred.append(action)\n",
        "\n",
        "pred = [pred[i][0] for i in range(len(test_s_scaled))]\n",
        "pred = pd.Series(pred)\n",
        "pred = pred*(A-B)+B\n",
        "actual = pd.Series(test_a)\n",
        "\n",
        "plt.scatter(pred,test_a,marker = '.')\n",
        "plt.xlabel('Predicted Value')\n",
        "plt.ylabel('Actual value')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "w_06UfNMwLdj",
        "34JMPZlKwPxj",
        "pOk7Y3-qXqkL",
        "Gejc1jM-u7x5",
        "BEcLwQd5vD2X",
        "dG4QppG8vV3_",
        "lmyyUm5rNbVs",
        "CU_TTlWbM0Aw",
        "8ad8kRLiyxx7",
        "cMmQZb3AzBxy",
        "yN_Yn-_dzJ_C",
        "TeHCn8Fg0E7g"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "private_outputs": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}